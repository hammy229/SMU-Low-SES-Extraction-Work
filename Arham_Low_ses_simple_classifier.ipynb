{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hammy229/SMU-Low-SES-Extraction-Work/blob/main/Arham_Low_ses_simple_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2RPslQkaal_5",
      "metadata": {
        "id": "2RPslQkaal_5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "b47d8bda-0676-4690-f560-f3c2c62d01aa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     case = d.expect([\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return self.expect_list(compiled_pattern_list,\n\u001b[0m\u001b[1;32m    355\u001b[0m                 timeout, searchwindowsize, async_)\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UXV4L725JHxh",
      "metadata": {
        "id": "UXV4L725JHxh"
      },
      "source": [
        "**Move to your shared file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SCrwEGbkavxw",
      "metadata": {
        "id": "SCrwEGbkavxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875cdd53-dd86-40b5-94a4-980f189a06cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/My Drive/SMU Work'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My Drive/SMU Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe94c56-4675-4f30-af2e-98e058c23169",
      "metadata": {
        "id": "6fe94c56-4675-4f30-af2e-98e058c23169"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path = \"GroundTruthFilter\"\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ac24b0-f20c-4757-bfe6-e94c2f8bcde7",
      "metadata": {
        "id": "15ac24b0-f20c-4757-bfe6-e94c2f8bcde7"
      },
      "outputs": [],
      "source": [
        "ground_truth_texts = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fKgtUSP-9eE5",
      "metadata": {
        "id": "fKgtUSP-9eE5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20394f86-db3e-4a81-8ae1-75cc0ed0125d",
      "metadata": {
        "id": "20394f86-db3e-4a81-8ae1-75cc0ed0125d"
      },
      "outputs": [],
      "source": [
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bdb4f15-ed76-49e8-bd4c-34687bcc1c15",
      "metadata": {
        "id": "3bdb4f15-ed76-49e8-bd4c-34687bcc1c15"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c4da4c3-29ae-4252-af51-a80897ef2718",
      "metadata": {
        "id": "8c4da4c3-29ae-4252-af51-a80897ef2718"
      },
      "outputs": [],
      "source": [
        "ground_truth_texts= [preprocess_text(item) for item in ground_truth_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c54ad00f-9235-4587-aa51-5807db5b7003",
      "metadata": {
        "id": "c54ad00f-9235-4587-aa51-5807db5b7003"
      },
      "outputs": [],
      "source": [
        "my_file = open(\"not-low-ses.txt\", \"r\")\n",
        "data = my_file.readlines()\n",
        "my_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd5a531-d089-4f1a-b970-1254a924ac4c",
      "metadata": {
        "id": "fcd5a531-d089-4f1a-b970-1254a924ac4c"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acea8e90-a483-4afd-b5e9-9ca0569ae10b",
      "metadata": {
        "id": "acea8e90-a483-4afd-b5e9-9ca0569ae10b"
      },
      "outputs": [],
      "source": [
        "not_low_ses = []\n",
        "for line in data:\n",
        "    if len(line)>100:\n",
        "        not_low_ses.append(line)\n",
        "    if len(not_low_ses) == len(ground_truth_texts):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53aa8ac-ca7e-468b-9080-159289be344b",
      "metadata": {
        "id": "e53aa8ac-ca7e-468b-9080-159289be344b"
      },
      "outputs": [],
      "source": [
        "len(not_low_ses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59804db8-89e2-4fe5-919b-a36990679166",
      "metadata": {
        "id": "59804db8-89e2-4fe5-919b-a36990679166"
      },
      "outputs": [],
      "source": [
        "not_low_ses_texts= [preprocess_text(item) for item in not_low_ses]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397bc84e-1696-4933-bbb0-1b28e420355f",
      "metadata": {
        "id": "397bc84e-1696-4933-bbb0-1b28e420355f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f8c898-cda4-47f1-8697-d09a86e80cc8",
      "metadata": {
        "id": "c6f8c898-cda4-47f1-8697-d09a86e80cc8"
      },
      "outputs": [],
      "source": [
        "all_text = ground_truth_texts + not_low_ses_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2287333b-c020-4ec6-b5e3-cd23b1ca2ac1",
      "metadata": {
        "id": "2287333b-c020-4ec6-b5e3-cd23b1ca2ac1"
      },
      "outputs": [],
      "source": [
        "len(all_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7a0baf-5892-4c72-ba01-a94f7e064a05",
      "metadata": {
        "id": "fd7a0baf-5892-4c72-ba01-a94f7e064a05"
      },
      "outputs": [],
      "source": [
        "all_label = [1 for i in range(len(ground_truth_texts))] + [-1 for i in range(len(not_low_ses_texts))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838be91f-1efe-4c4a-a0de-91f2666e9230",
      "metadata": {
        "id": "838be91f-1efe-4c4a-a0de-91f2666e9230"
      },
      "outputs": [],
      "source": [
        "len(all_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e089108f-db37-4d68-a85c-cb5f2f2a954f",
      "metadata": {
        "id": "e089108f-db37-4d68-a85c-cb5f2f2a954f"
      },
      "outputs": [],
      "source": [
        "# all_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1QFL8vphJm5y",
      "metadata": {
        "id": "1QFL8vphJm5y"
      },
      "source": [
        "# **Simple Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NctJv1bHiX0x",
      "metadata": {
        "id": "NctJv1bHiX0x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "all_text = np.array(all_text)\n",
        "all_label = np.array(all_label)\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_text, all_label, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TR7uLgjGJrGg",
      "metadata": {
        "id": "TR7uLgjGJrGg"
      },
      "outputs": [],
      "source": [
        "vectorizer.fit(train_texts)\n",
        "X_train = vectorizer.transform(train_texts)\n",
        "X_test  = vectorizer.transform(val_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90Mhu1DdJvBR",
      "metadata": {
        "id": "90Mhu1DdJvBR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, train_labels)\n",
        "score = classifier.score(X_test, val_labels)\n",
        "\n",
        "print(\"Accuracy:\", score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pJ47NYkfJziP",
      "metadata": {
        "id": "pJ47NYkfJziP"
      },
      "outputs": [],
      "source": [
        "classifier.predict_proba(X_train[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"it's a nice place. if you don't have friends it can be hard to find fun things to do on the weekends, so definitely make an effort early on to meet interesting people so that you aren't stuck alone in your dorm on the weekends. it pretty easy to navigate, but there is an nku app you should definitely download that you can use for a lot of things like looking at grades and bus schedules. if you don't have a car learn the bus lines. leave campus often to explore cincinnati. campus can be quiet sometimes, but you will never run out of fun things to do in the city. join clubs specific to your major to meet people. intramurals are a pretty big part of greek life here as well. walk around buildings where you don't have classes, you never know what you might stumble upon. the dry campus policy is pretty strict, so if you want to drink try to find some friends with apartments off campus. people tend to be nice and helpful. if you have a meal plan, einsteins in the library is the best place to get lunch, but they only accept swipes after 1 pm. the norse tech bar is awesome, utilize it. they can fix your computer, and you can even check out equipment such as cameras and ipads. that's all i can think of for now, hope you enjoy nku.\"\n"
      ],
      "metadata": {
        "id": "GCnQdq6gGc4e"
      },
      "id": "GCnQdq6gGc4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LRPredict():\n",
        "    predictions = []\n",
        "    for i, text in enumerate(ground_truth_texts):\n",
        "        text = preprocess_text(text)\n",
        "        vec_text = vectorizer.transform([text])\n",
        "        prob = classifier.predict_proba(vec_text)\n",
        "        if prob[0][1]>0.90:\n",
        "            predictions.append(f\"Text {i+1}: low_ses\")\n",
        "        else:\n",
        "            predictions.append(f\"Text {i+1}: not_low_ses\")\n",
        "    return predictions\n",
        "\n",
        "predictions = LRPredict()\n",
        "for prediction in predictions:\n",
        "    print(prediction)"
      ],
      "metadata": {
        "id": "zw37UneFFYtA"
      },
      "id": "zw37UneFFYtA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorrect classification for Now_Low_Ses - 5, 12, etc"
      ],
      "metadata": {
        "id": "n7-59cZdF39Y"
      },
      "id": "n7-59cZdF39Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3twFSmVsJ2r5",
      "metadata": {
        "id": "3twFSmVsJ2r5"
      },
      "outputs": [],
      "source": [
        "def LRPredict(text):\n",
        "    text = preprocess_text(text)\n",
        "    vec_text = vectorizer.transform([text])\n",
        "    prob = classifier.predict_proba(vec_text)\n",
        "    if prob[0][1]>0.90:\n",
        "        return \"low_ses\"\n",
        "    else:\n",
        "        return \"not_low_ses\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D4yvf3UVJ6at",
      "metadata": {
        "id": "D4yvf3UVJ6at"
      },
      "outputs": [],
      "source": [
        "LRPredict(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def classify_ses(text):\n",
        "    low_ses_keywords = [\n",
        "        \"poor\", \"low income\", \"unemployed\", \"financial aid\", \"pell grant\",\n",
        "        \"food pantry\", \"working multiple jobs\", \"low wage\", \"scholarships\",\n",
        "        \"struggling to pay\", \"couldn't afford to\", \"no money for\", \"in debt\", \"relying on grants\",\n",
        "        \"parents can't afford to\", \"financial hardship\", \"living in poverty\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in low_ses_keywords:\n",
        "        if keyword in text_lower:\n",
        "            return \"Low SES\"\n",
        "\n",
        "    return \"Not Low SES\"\n",
        "\n",
        "def process_files(directory):\n",
        "    results = {}\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                # Split the file content by newlines and remove any empty entries\n",
        "                entries = [entry for entry in content.split('\\n') if entry.strip()]  # Filter out empty entries\n",
        "                entry_results = []\n",
        "                for i, entry in enumerate(entries):\n",
        "                    classification = classify_ses(entry)\n",
        "                    entry_results.append((i+1, classification, entry[:100]))  # Store entry index, classification, and snippet\n",
        "                    print(f\"Processing entry {i+1} in file {filename}\")\n",
        "                    print(f\"Content snippet: {entry[:100]}...\")  # Print first 100 characters for context\n",
        "                    print(f\"Classification: {classification}\")\n",
        "                results[filename] = entry_results\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def main():\n",
        "    directory = \"/content/drive/My Drive/SMU Work\"\n",
        "    results = process_files(directory)\n",
        "\n",
        "    for filename, entries in results.items():\n",
        "        print(f\"\\nResults for {filename}:\")\n",
        "        for index, classification, snippet in entries:\n",
        "            print(f\"Entry {index}: {classification} - Snippet: {snippet}...\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "w72GIGoh--J_"
      },
      "id": "w72GIGoh--J_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My Drive/SMU Work\n",
        "\n",
        "import os\n",
        "path = \"GroundTruthFilter\"\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]\n",
        "\n",
        "ground_truth_texts = []\n",
        "\n",
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "ground_truth_texts= [preprocess_text(item) for item in ground_truth_texts]\n",
        "\n",
        "my_file = open(\"not-low-ses.txt\", \"r\")\n",
        "data = my_file.readlines()\n",
        "my_file.close()\n",
        "\n",
        "not_low_ses = []\n",
        "for line in data:\n",
        "    if len(line)>100:\n",
        "        not_low_ses.append(line)\n",
        "    if len(not_low_ses) == len(ground_truth_texts):\n",
        "        break\n",
        "\n",
        "not_low_ses_texts= [preprocess_text(item) for item in not_low_ses]\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_text = ground_truth_texts + not_low_ses_texts\n",
        "all_label = [1 for i in range(len(ground_truth_texts))] + [-1 for i in range(len(not_low_ses_texts))]\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_text, all_label, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_texts)\n",
        "X_train = vectorizer.transform(train_texts)\n",
        "X_test  = vectorizer.transform(val_texts)\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rfc.fit(X_train, train_labels)\n",
        "\n",
        "score = rfc.score(X_test, val_labels)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "def LRPredict(text):\n",
        "    text = preprocess_text(text)\n",
        "    vec_text = vectorizer.transform([text])\n",
        "    prob = rfc.predict(vec_text)\n",
        "    if prob[0]==1:\n",
        "        return \"low_ses\"\n",
        "    else:\n",
        "        return \"not_low_ses\"\n",
        "\n",
        "text = \"it's a nice place. if you don't have friends it can be hard to find fun things to do on the weekends, so definitely make an effort early on to meet interesting people so that you aren't stuck alone in your dorm on the weekends. it pretty easy to navigate, but there is an nku app you should definitely download that you can use for a lot of things like looking at grades and bus schedules. if you don't have a car learn the bus lines. leave campus often to explore cincinnati. campus can be quiet sometimes, but you will never run out of fun things to do in the city. join clubs specific to your major to meet people. intramurals are a pretty big part of greek life here as well. walk around buildings where you don't have classes, you never know what you might stumble upon. the dry campus policy is pretty strict, so if you want to drink try to find some friends with apartments off campus. people tend to be nice and helpful. if you have a meal plan, einsteins in the library is the best place to get lunch, but they only accept swipes after 1 pm. the norse tech bar is awesome, utilize it. they can fix your computer, and you can even check out equipment such as cameras and ipads. that's all i can think of for now, hope you enjoy nku.\"\n",
        "\n",
        "print(LRPredict(text))\n",
        "\n",
        "def classify_ses(text):\n",
        "    low_ses_keywords = [\n",
        "        \"poor\", \"low income\", \"unemployed\", \"financial aid\", \"pell grant\",\n",
        "        \"food pantry\", \"working multiple jobs\", \"low wage\", \"scholarships\",\n",
        "        \"struggling to pay\", \"couldn't afford to\", \"no money for\", \"in debt\", \"relying on grants\",\n",
        "        \"parents can't afford to\", \"financial hardship\", \"living in poverty\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in low_ses_keywords:\n",
        "        if keyword in text_lower:\n",
        "            return \"Low SES\"\n",
        "\n",
        "    return \"Not Low SES\"\n",
        "\n",
        "def process_files(directory):\n",
        "    results = {}\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxgu1I-7iM1q",
        "outputId": "25ad977c-6271-4e50-f017-d644680aeb69"
      },
      "id": "Uxgu1I-7iM1q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/SMU Work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6538461538461539\n",
            "not_low_ses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My Drive/SMU Work\n",
        "\n",
        "import os\n",
        "path = \"GroundTruthFilter\"\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]\n",
        "\n",
        "ground_truth_texts = []\n",
        "\n",
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "ground_truth_texts= [preprocess_text(item) for item in ground_truth_texts]\n",
        "\n",
        "my_file = open(\"not-low-ses.txt\", \"r\")\n",
        "data = my_file.readlines()\n",
        "my_file.close()\n",
        "\n",
        "not_low_ses = []\n",
        "for line in data:\n",
        "    if len(line)>100:\n",
        "        not_low_ses.append(line)\n",
        "    if len(not_low_ses) == len(ground_truth_texts):\n",
        "        break\n",
        "\n",
        "not_low_ses_texts= [preprocess_text(item) for item in not_low_ses]\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_text = ground_truth_texts + not_low_ses_texts\n",
        "all_label = [1 for i in range(len(ground_truth_texts))] + [-1 for i in range(len(not_low_ses_texts))]\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_text, all_label, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_texts)\n",
        "X_train = vectorizer.transform(train_texts)\n",
        "X_test  = vectorizer.transform(val_texts)\n",
        "\n",
        "svm_model = svm.SVC(kernel='linear', C=1)\n",
        "svm_model.fit(X_train, train_labels)\n",
        "\n",
        "score = svm_model.score(X_test, val_labels)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "def LRPredict(text):\n",
        "    text = preprocess_text(text)\n",
        "    vec_text = vectorizer.transform([text])\n",
        "    prob = svm_model.predict(vec_text)\n",
        "    if prob[0]==1:\n",
        "        return \"low_ses\"\n",
        "    else:\n",
        "        return \"not_low_ses\"\n",
        "\n",
        "text = \"it's a nice place. if you don't have friends it can be hard to find fun things to do on the weekends, so definitely make an effort early on to meet interesting people so that you aren't stuck alone in your dorm on the weekends. it pretty easy to navigate, but there is an nku app you should definitely download that you can use for a lot of things like looking at grades and bus schedules. if you don't have a car learn the bus lines. leave campus often to explore cincinnati. campus can be quiet sometimes, but you will never run out of fun things to do in the city. join clubs specific to your major to meet people. intramurals are a pretty big part of greek life here as well. walk around buildings where you don't have classes, you never know what you might stumble upon. the dry campus policy is pretty strict, so if you want to drink try to find some friends with apartments off campus. people tend to be nice and helpful. if you have a meal plan, einsteins in the library is the best place to get lunch, but they only accept swipes after 1 pm. the norse tech bar is awesome, utilize it. they can fix your computer, and you can even check out equipment such as cameras and ipads. that's all i can think of for now, hope you enjoy nku.\"\n",
        "\n",
        "print(LRPredict(text))\n",
        "\n",
        "def classify_ses(text):\n",
        "    low_ses_keywords = [\n",
        "        \"poor\", \"low income\", \"unemployed\", \"financial aid\", \"pell grant\",\n",
        "        \"food pantry\", \"working multiple jobs\", \"low wage\", \"scholarships\",\n",
        "        \"struggling to pay\", \"couldn't afford to\", \"no money for\", \"in debt\", \"relying on grants\",\n",
        "        \"parents can't afford to\", \"financial hardship\", \"living in poverty\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in low_ses_keywords:\n",
        "        if keyword in text_lower:\n",
        "            return \"Low SES\"\n",
        "\n",
        "    return \"Not Low SES\"\n",
        "\n",
        "def process_files(directory):\n",
        "    results = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            # Add your logic here to process the file\n",
        "            pass # This is a placeholder, replace it with your file processing code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqboNqdIjpab",
        "outputId": "f4917c81-ed16-4dbb-d602-5f647a7dd108"
      },
      "id": "NqboNqdIjpab",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/SMU Work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8076923076923077\n",
            "not_low_ses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHuLbEAYkybR"
      },
      "id": "iHuLbEAYkybR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}