{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hammy229/SMU-Low-SES-Extraction-Work/blob/main/Arham_SMU_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2RPslQkaal_5",
      "metadata": {
        "id": "2RPslQkaal_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2c04d3-803f-4986-aa9c-85117d1ead7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UXV4L725JHxh",
      "metadata": {
        "id": "UXV4L725JHxh"
      },
      "source": [
        "**Move to your shared file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SCrwEGbkavxw",
      "metadata": {
        "id": "SCrwEGbkavxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782cfe78-3c25-4cdb-908a-f6b76b3a2171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/SMU Work\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My Drive/SMU Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe94c56-4675-4f30-af2e-98e058c23169",
      "metadata": {
        "id": "6fe94c56-4675-4f30-af2e-98e058c23169"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path = \"GroundTruthFilter\"\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"GroundTruthFilter\"  # assume this is the folder containing your ground truth files\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]\n",
        "\n",
        "ground_truth_texts = []\n",
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)"
      ],
      "metadata": {
        "id": "gP8zo68136aV"
      },
      "id": "gP8zo68136aV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ac24b0-f20c-4757-bfe6-e94c2f8bcde7",
      "metadata": {
        "id": "15ac24b0-f20c-4757-bfe6-e94c2f8bcde7"
      },
      "outputs": [],
      "source": [
        "ground_truth_texts = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ground_truth_texts)"
      ],
      "metadata": {
        "id": "aPwJtFSJ3QMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d04e8f-9955-4fac-e376-7ce15b3fa355"
      },
      "id": "aPwJtFSJ3QMJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fKgtUSP-9eE5",
      "metadata": {
        "id": "fKgtUSP-9eE5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20394f86-db3e-4a81-8ae1-75cc0ed0125d",
      "metadata": {
        "id": "20394f86-db3e-4a81-8ae1-75cc0ed0125d"
      },
      "outputs": [],
      "source": [
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:  # Use os.path.join to construct the correct file path\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bdb4f15-ed76-49e8-bd4c-34687bcc1c15",
      "metadata": {
        "id": "3bdb4f15-ed76-49e8-bd4c-34687bcc1c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d9a572-5684-4744-fbe9-e84ee8b2d73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# Preprocessing text data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')  # Ensure 'punkt' is downloaded\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c4da4c3-29ae-4252-af51-a80897ef2718",
      "metadata": {
        "id": "8c4da4c3-29ae-4252-af51-a80897ef2718"
      },
      "outputs": [],
      "source": [
        "ground_truth_texts= [preprocess_text(item) for item in ground_truth_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c54ad00f-9235-4587-aa51-5807db5b7003",
      "metadata": {
        "id": "c54ad00f-9235-4587-aa51-5807db5b7003"
      },
      "outputs": [],
      "source": [
        "my_file = open(\"not-low-ses.txt\", \"r\")\n",
        "data = my_file.readlines()\n",
        "my_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd5a531-d089-4f1a-b970-1254a924ac4c",
      "metadata": {
        "id": "fcd5a531-d089-4f1a-b970-1254a924ac4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50abb55-228b-46e0-e25d-1dbf49ceedd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "468"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acea8e90-a483-4afd-b5e9-9ca0569ae10b",
      "metadata": {
        "id": "acea8e90-a483-4afd-b5e9-9ca0569ae10b"
      },
      "outputs": [],
      "source": [
        "not_low_ses = []\n",
        "for line in data:\n",
        "    if len(line)>100:\n",
        "        not_low_ses.append(line)\n",
        "    if len(not_low_ses) == len(ground_truth_texts):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53aa8ac-ca7e-468b-9080-159289be344b",
      "metadata": {
        "id": "e53aa8ac-ca7e-468b-9080-159289be344b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f89af20-0e03-46cc-ac8f-04c1b785f5f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(not_low_ses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59804db8-89e2-4fe5-919b-a36990679166",
      "metadata": {
        "id": "59804db8-89e2-4fe5-919b-a36990679166"
      },
      "outputs": [],
      "source": [
        "not_low_ses_texts= [preprocess_text(item) for item in not_low_ses]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397bc84e-1696-4933-bbb0-1b28e420355f",
      "metadata": {
        "id": "397bc84e-1696-4933-bbb0-1b28e420355f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f8c898-cda4-47f1-8697-d09a86e80cc8",
      "metadata": {
        "id": "c6f8c898-cda4-47f1-8697-d09a86e80cc8"
      },
      "outputs": [],
      "source": [
        "all_text = ground_truth_texts + not_low_ses_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2287333b-c020-4ec6-b5e3-cd23b1ca2ac1",
      "metadata": {
        "id": "2287333b-c020-4ec6-b5e3-cd23b1ca2ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884e5bea-4cb6-4786-9d74-c6be2f1b0ad3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(all_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7a0baf-5892-4c72-ba01-a94f7e064a05",
      "metadata": {
        "id": "fd7a0baf-5892-4c72-ba01-a94f7e064a05"
      },
      "outputs": [],
      "source": [
        "all_label = [1 for i in range(len(ground_truth_texts))] + [-1 for i in range(len(not_low_ses_texts))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838be91f-1efe-4c4a-a0de-91f2666e9230",
      "metadata": {
        "id": "838be91f-1efe-4c4a-a0de-91f2666e9230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca995d48-5024-48e3-8da5-69d028956577"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(all_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e089108f-db37-4d68-a85c-cb5f2f2a954f",
      "metadata": {
        "id": "e089108f-db37-4d68-a85c-cb5f2f2a954f"
      },
      "outputs": [],
      "source": [
        "# all_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1QFL8vphJm5y",
      "metadata": {
        "id": "1QFL8vphJm5y"
      },
      "source": [
        "# **Simple Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NctJv1bHiX0x",
      "metadata": {
        "id": "NctJv1bHiX0x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "all_text = np.array(all_text)\n",
        "all_label = np.array(all_label)\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_text, all_label, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TR7uLgjGJrGg",
      "metadata": {
        "id": "TR7uLgjGJrGg"
      },
      "outputs": [],
      "source": [
        "vectorizer.fit(train_texts)\n",
        "X_train = vectorizer.transform(train_texts)\n",
        "X_test  = vectorizer.transform(val_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90Mhu1DdJvBR",
      "metadata": {
        "id": "90Mhu1DdJvBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447b4c5f-2ca7-49f3-f8a3-2e8cb946c325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8076923076923077\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, train_labels)\n",
        "score = classifier.score(X_test, val_labels)\n",
        "\n",
        "print(\"Accuracy:\", score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pJ47NYkfJziP",
      "metadata": {
        "id": "pJ47NYkfJziP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474a0c2d-faa3-4d0f-e5fe-ed6ed55e5831"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.975454  , 0.024546  ],\n",
              "       [0.98770356, 0.01229644],\n",
              "       [0.0072068 , 0.9927932 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "classifier.predict_proba(X_train[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"A lot of people around me at this school come from families that live comfortably. I, however, am poor. Like my mom makes less than 30k a year. I feel like people are hearing me but they’re not listening. We’re also at a disadvantage because the schooling in my area isn’t as rigorous as LA or Bay Area kids. Nonetheless, I’m here. My tuition or room and board is expensive. Not even 33k worth of grants and scholarships and loans could pay for all of my school. And I also didn’t come to college with a laptop. I don’t have enough money for books. How the hell am I supposed to be an engineer without a laptop. That’s like the 1 thing I need. This sucks hard. It’s a fun adventure, but the real world is fucking brutal.\""
      ],
      "metadata": {
        "id": "EQHuYGu95iXo"
      },
      "id": "EQHuYGu95iXo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3twFSmVsJ2r5",
      "metadata": {
        "id": "3twFSmVsJ2r5"
      },
      "outputs": [],
      "source": [
        "def LRPredict(text):\n",
        "    text = preprocess_text(text)\n",
        "    vec_text = vectorizer.transform([text])\n",
        "    prob = classifier.predict_proba(vec_text)\n",
        "    if prob[0][1]>0.90:\n",
        "        return \"low_ses\"\n",
        "    else:\n",
        "        return \"not_low_ses\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D4yvf3UVJ6at",
      "metadata": {
        "id": "D4yvf3UVJ6at",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b3c105c-49da-45ce-addd-ffa990077e33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'low_ses'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "LRPredict(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def LRPredict(text):\n",
        "    text = preprocess_text(text)\n",
        "    vec_text = vectorizer.transform([text])\n",
        "    prob = classifier.predict_proba(vec_text)[0][1]\n",
        "    return prob\n",
        "\n",
        "text = \"My college had a student run food bank on campus (actually was pretty awesome, I volunteered there a bunch when I could.) We also got free bus/train passes, free gym membership, free food and tshirts at events at least once a week, career center workshops like resume building, free tutoring services, free editing for essays, even a student run “fix it Friday” where people would help fix broken phones and stuff. Healthcare center gave free condoms. Our library would give free scantrons and blue books (required for exams and can be like 10 bucks each) if you asked nicely.\"\n",
        "ses_score = LRPredict(text)\n",
        "print(\"SES Score:\", ses_score)\n",
        "\n",
        "if ses_score < 0.3:\n",
        "    print(\"Very low SES\")\n",
        "elif ses_score < 0.6:\n",
        "    print(\"Low SES\")\n",
        "elif ses_score < 0.8:\n",
        "    print(\"Moderate SES\")\n",
        "else:\n",
        "    print(\"High SES\")"
      ],
      "metadata": {
        "id": "eZxCQleP5bzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1af5b3-b316-49b1-c916-00e01680210e"
      },
      "id": "eZxCQleP5bzQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SES Score: 0.1584767874258222\n",
            "Very low SES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My Drive/SMU Work\n",
        "\n",
        "import os\n",
        "path = \"GroundTruthFilter\"\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]\n",
        "\n",
        "ground_truth_texts = []\n",
        "\n",
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "ground_truth_texts = [preprocess_text(item) for item in ground_truth_texts]\n",
        "\n",
        "my_file = open(\"not-low-ses.txt\", \"r\")\n",
        "data = my_file.readlines()\n",
        "my_file.close()\n",
        "\n",
        "not_low_ses = []\n",
        "for line in data:\n",
        "    if len(line) > 100:\n",
        "        not_low_ses.append(line)\n",
        "    if len(not_low_ses) == len(ground_truth_texts):\n",
        "        break\n",
        "\n",
        "not_low_ses_texts = [preprocess_text(item) for item in not_low_ses]\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine the ground truth and not low SES texts into a single dataset\n",
        "all_text = ground_truth_texts + not_low_ses_texts\n",
        "all_label = [1 for i in range(len(ground_truth_texts))] + [-1 for i in range(len(not_low_ses_texts))]\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_text, all_label, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_texts)\n",
        "X_train = vectorizer.transform(train_texts)\n",
        "X_test = vectorizer.transform(val_texts)\n",
        "\n",
        "# Initialize and fit the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, train_labels)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "score = rf_model.score(X_test, val_labels)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "def RFPredict(text):\n",
        "    text = preprocess_text(text)\n",
        "    vec_text = vectorizer.transform([text])\n",
        "    prob = rf_model.predict(vec_text)\n",
        "    if prob[0] == 1:\n",
        "        return \"low_ses\"\n",
        "    else:\n",
        "        return \"not_low_ses\"\n",
        "\n",
        "# Example text prediction\n",
        "text = \"it's a nice place. if you don't have friends it can be hard to find fun things to do on the weekends, so definitely make an effort early on to meet interesting people so that you aren't stuck alone in your dorm on the weekends.\"\n",
        "print(RFPredict(text))\n",
        "\n",
        "def classify_ses(text):\n",
        "    low_ses_keywords = [\n",
        "        \"poor\", \"low income\", \"unemployed\", \"financial aid\", \"pell grant\",\n",
        "        \"food pantry\", \"working multiple jobs\", \"low wage\", \"scholarships\",\n",
        "        \"struggling to pay\", \"couldn't afford to\", \"no money for\", \"in debt\", \"relying on grants\",\n",
        "        \"parents can't afford to\", \"financial hardship\", \"living in poverty\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in low_ses_keywords:\n",
        "        if keyword in text_lower:\n",
        "            return \"Low SES\"\n",
        "\n",
        "    return \"Not Low SES\"\n",
        "\n",
        "def process_files(directory):\n",
        "    results = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(directory, filename), 'r') as file:\n",
        "                content = file.read()\n",
        "                result = classify_ses(content)\n",
        "                results[filename] = result\n",
        "    return results\n",
        "\n",
        "# Example of processing a directory of files\n",
        "directory_path = \"/content/drive/My Drive/SMU Work\"\n",
        "results = process_files(directory_path)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G92u2TeSmW1i",
        "outputId": "441cd8b8-b17a-4dab-8874-7b3ed8c6606c"
      },
      "id": "G92u2TeSmW1i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/SMU Work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6538461538461539\n",
            "not_low_ses\n",
            "{'not-low-ses.txt': 'Low SES'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My Drive/SMU Work\n",
        "\n",
        "import os\n",
        "path = \"GroundTruthFilter\"\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]\n",
        "\n",
        "ground_truth_texts = []\n",
        "\n",
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "ground_truth_texts = [preprocess_text(item) for item in ground_truth_texts]\n",
        "\n",
        "my_file = open(\"not-low-ses.txt\", \"r\")\n",
        "data = my_file.readlines()\n",
        "my_file.close()\n",
        "\n",
        "not_low_ses = []\n",
        "for line in data:\n",
        "    if len(line) > 100:\n",
        "        not_low_ses.append(line)\n",
        "    if len(not_low_ses) == len(ground_truth_texts):\n",
        "        break\n",
        "\n",
        "not_low_ses_texts = [preprocess_text(item) for item in not_low_ses]\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine the ground truth and not low SES texts into a single dataset\n",
        "all_text = ground_truth_texts + not_low_ses_texts\n",
        "all_label = [1 for i in range(len(ground_truth_texts))] + [-1 for i in range(len(not_low_ses_texts))]\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_text, all_label, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_texts)\n",
        "X_train = vectorizer.transform(train_texts)\n",
        "X_test = vectorizer.transform(val_texts)\n",
        "\n",
        "# Initialize and fit the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, train_labels)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "score = nb_model.score(X_test, val_labels)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "def NBPredict(text):\n",
        "    text = preprocess_text(text)\n",
        "    vec_text = vectorizer.transform([text])\n",
        "    prob = nb_model.predict(vec_text)\n",
        "    if prob[0] == 1:\n",
        "        return \"low_ses\"\n",
        "    else:\n",
        "        return \"not_low_ses\"\n",
        "\n",
        "# Example text prediction\n",
        "text = \"it's a nice place. if you don't have friends it can be hard to find fun things to do on the weekends, so definitely make an effort early on to meet interesting people so that you aren't stuck alone in your dorm on the weekends.\"\n",
        "print(NBPredict(text))\n",
        "\n",
        "def classify_ses(text):\n",
        "    low_ses_keywords = [\n",
        "        \"poor\", \"low income\", \"unemployed\", \"financial aid\", \"pell grant\",\n",
        "        \"food pantry\", \"working multiple jobs\", \"low wage\", \"scholarships\",\n",
        "        \"struggling to pay\", \"couldn't afford to\", \"no money for\", \"in debt\", \"relying on grants\",\n",
        "        \"parents can't afford to\", \"financial hardship\", \"living in poverty\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in low_ses_keywords:\n",
        "        if keyword in text_lower:\n",
        "            return \"Low SES\"\n",
        "\n",
        "    return \"Not Low SES\"\n",
        "\n",
        "def process_files(directory):\n",
        "    results = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(directory, filename), 'r') as file:\n",
        "                content = file.read()\n",
        "                result = classify_ses(content)\n",
        "                results[filename] = result\n",
        "    return results\n",
        "\n",
        "# Example of processing a directory of files\n",
        "directory_path = \"/content/drive/My Drive/SMU Work\"\n",
        "results = process_files(directory_path)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA47NaAKnW43",
        "outputId": "a5fadf1a-0811-4d62-8a57-c7a69ef94f35"
      },
      "id": "uA47NaAKnW43",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/SMU Work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8076923076923077\n",
            "not_low_ses\n",
            "{'not-low-ses.txt': 'Low SES'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My Drive/SMU Work\n",
        "\n",
        "import os\n",
        "path = \"GroundTruthFilter\"\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]\n",
        "\n",
        "ground_truth_texts = []\n",
        "\n",
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization and lowercasing\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "ground_truth_texts = [preprocess_text(item) for item in ground_truth_texts]\n",
        "\n",
        "my_file = open(\"not-low-ses.txt\", \"r\")\n",
        "data = my_file.readlines()\n",
        "my_file.close()\n",
        "\n",
        "not_low_ses = []\n",
        "for line in data:\n",
        "    if len(line) > 100:\n",
        "        not_low_ses.append(line)\n",
        "    if len(not_low_ses) == len(ground_truth_texts):\n",
        "        break\n",
        "\n",
        "not_low_ses_texts = [preprocess_text(item) for item in not_low_ses]\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine the ground truth and not low SES texts into a single dataset\n",
        "all_text = ground_truth_texts + not_low_ses_texts\n",
        "all_label = [1 for i in range(len(ground_truth_texts))] + [-1 for i in range(len(not_low_ses_texts))]\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_text, all_label, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_texts)\n",
        "X_train = vectorizer.transform(train_texts)\n",
        "X_test = vectorizer.transform(val_texts)\n",
        "\n",
        "# Initialize and fit the SVM model\n",
        "svm_model = SVC(kernel='linear', C=1, random_state=42)\n",
        "svm_model.fit(X_train, train_labels)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "score = svm_model.score(X_test, val_labels)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "def SVMPredict(text):\n",
        "    text = preprocess_text(text)\n",
        "    vec_text = vectorizer.transform([text])\n",
        "    prob = svm_model.predict(vec_text)\n",
        "    if prob[0] == 1:\n",
        "        return \"low_ses\"\n",
        "    else:\n",
        "        return \"not_low_ses\"\n",
        "\n",
        "# Example text prediction\n",
        "text = \"it's a nice place. if you don't have friends it can be hard to find fun things to do on the weekends, so definitely make an effort early on to meet interesting people so that you aren't stuck alone in your dorm on the weekends.\"\n",
        "print(SVMPredict(text))\n",
        "\n",
        "def classify_ses(text):\n",
        "    low_ses_keywords = [\n",
        "        \"poor\", \"low income\", \"unemployed\", \"financial aid\", \"pell grant\",\n",
        "        \"food pantry\", \"working multiple jobs\", \"low wage\", \"scholarships\",\n",
        "        \"struggling to pay\", \"couldn't afford to\", \"no money for\", \"in debt\", \"relying on grants\",\n",
        "        \"parents can't afford to\", \"financial hardship\", \"living in poverty\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in low_ses_keywords:\n",
        "        if keyword in text_lower:\n",
        "            return \"Low SES\"\n",
        "\n",
        "    return \"Not Low SES\"\n",
        "\n",
        "def process_files(directory):\n",
        "    results = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(directory, filename), 'r') as file:\n",
        "                content = file.read()\n",
        "                result = classify_ses(content)\n",
        "                results[filename] = result\n",
        "    return results\n",
        "\n",
        "# Example of processing a directory of files\n",
        "directory_path = \"/content/drive/My Drive/SMU Work\"\n",
        "results = process_files(directory_path)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwL0ieuBnphz",
        "outputId": "edff1070-8fcb-442c-88e4-ac1618be9788"
      },
      "id": "wwL0ieuBnphz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/SMU Work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8076923076923077\n",
            "not_low_ses\n",
            "{'not-low-ses.txt': 'Low SES'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory to the SMU Work folder in Google Drive\n",
        "%cd /content/drive/My Drive/SMU Work\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (stopwords and tokenizer)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocessing function to clean and tokenize text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Convert text to lowercase and tokenize\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Keep only alphabetic tokens (remove numbers, punctuation, etc.)\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords (common words like \"and\", \"the\", etc.)\n",
        "    return ' '.join(tokens)  # Join tokens back into a single string\n",
        "\n",
        "# Load Ground Truth Texts (low SES-related texts)\n",
        "path = \"GroundTruthFilter\"\n",
        "files = [file for file in os.listdir(path) if not file.startswith('.')]\n",
        "\n",
        "ground_truth_texts = []\n",
        "for file_name in files:\n",
        "    with open(os.path.join(path, file_name), 'r') as f:\n",
        "        output = f.read()\n",
        "        ground_truth_texts.append(output)\n",
        "\n",
        "# Preprocess ground truth texts\n",
        "ground_truth_texts = [preprocess_text(item) for item in ground_truth_texts]\n",
        "\n",
        "# Load texts that are not low SES-related\n",
        "my_file = open(\"not-low-ses.txt\", \"r\")\n",
        "data = my_file.readlines()\n",
        "my_file.close()\n",
        "\n",
        "not_low_ses_texts = []\n",
        "for line in data:\n",
        "    if len(line) > 100:  # Filter lines with length greater than 100 characters\n",
        "        not_low_ses_texts.append(line)\n",
        "    if len(not_low_ses_texts) == len(ground_truth_texts):  # Ensure both categories have the same number of texts\n",
        "        break\n",
        "\n",
        "# Preprocess not low SES texts\n",
        "not_low_ses_texts = [preprocess_text(item) for item in not_low_ses_texts]\n",
        "\n",
        "# Combine both categories of texts and create labels\n",
        "all_texts = ground_truth_texts + not_low_ses_texts\n",
        "all_labels = [1] * len(ground_truth_texts) + [0] * len(not_low_ses_texts)  # 1 for low SES, 0 for not low SES\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_texts, all_labels, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Tokenization and padding of sequences\n",
        "vocab_size = 10000  # Maximum vocabulary size (number of unique words)\n",
        "max_length = 200  # Maximum length of each sequence (number of words per text)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")  # Create tokenizer, using <OOV> for out-of-vocabulary words\n",
        "tokenizer.fit_on_texts(train_texts)  # Fit the tokenizer on the training texts (learn the vocabulary)\n",
        "\n",
        "# Convert the texts to sequences of integers (tokenized form)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "# Pad the sequences to ensure uniform length (truncating or padding as necessary)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Define a custom callback to print only the epoch, accuracy, and dataset being trained on\n",
        "class PrintEpochCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"Epoch {epoch + 1}: Training Accuracy: {logs['accuracy']:.4f}, Validation Accuracy: {logs['val_accuracy']:.4f}\")\n",
        "\n",
        "# LSTM Model for text classification\n",
        "embedding_dim = 100  # Dimensionality of the embedding layer (size of the word vectors)\n",
        "model_lstm = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),  # Embedding layer for converting words to vectors\n",
        "    LSTM(64, return_sequences=False),  # LSTM layer with 64 units (good for capturing sequential information in text)\n",
        "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    Dense(32, activation='relu'),  # Dense (fully connected) layer with 32 units and ReLU activation\n",
        "    Dropout(0.5),  # Another dropout layer for regularization\n",
        "    Dense(1, activation='sigmoid')  # Output layer with a single unit and sigmoid activation for binary classification (1 or 0)\n",
        "])\n",
        "\n",
        "# Compile the LSTM model using binary cross-entropy loss and the Adam optimizer\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model on the training data, validating on the validation data, with custom callback\n",
        "history_lstm = model_lstm.fit(train_padded, np.array(train_labels), epochs=10, validation_data=(val_padded, np.array(val_labels)), batch_size=32, callbacks=[PrintEpochCallback()])\n",
        "\n",
        "# CNN Model for text classification\n",
        "model_cnn = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),  # Embedding layer\n",
        "    Conv1D(128, 5, activation='relu'),  # 1D Convolutional layer with 128 filters and kernel size of 5\n",
        "    MaxPooling1D(pool_size=2),  # Max pooling layer to down-sample the output of the convolutional layer\n",
        "    Conv1D(128, 5, activation='relu'),  # Another 1D Convolutional layer\n",
        "    GlobalMaxPooling1D(),  # Global max pooling layer to reduce the output to a single vector\n",
        "    Dropout(0.5),  # Dropout layer for regularization\n",
        "    Dense(32, activation='relu'),  # Dense layer with 32 units and ReLU activation\n",
        "    Dropout(0.5),  # Another dropout layer for regularization\n",
        "    Dense(1, activation='sigmoid')  # Output layer with a single unit and sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the CNN model using binary cross-entropy loss and the Adam optimizer\n",
        "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the CNN model on the training data, validating on the validation data, with custom callback\n",
        "history_cnn = model_cnn.fit(train_padded, np.array(train_labels), epochs=10, validation_data=(val_padded, np.array(val_labels)), batch_size=32, callbacks=[PrintEpochCallback()])\n",
        "\n",
        "# Evaluate the LSTM model on the validation data\n",
        "loss_lstm, accuracy_lstm = model_lstm.evaluate(val_padded, np.array(val_labels))\n",
        "print(f\"LSTM Model Final Accuracy: {accuracy_lstm:.4f}\")\n",
        "\n",
        "# Evaluate the CNN model on the validation data\n",
        "loss_cnn, accuracy_cnn = model_cnn.evaluate(val_padded, np.array(val_labels))\n",
        "print(f\"CNN Model Final Accuracy: {accuracy_cnn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyLMVCtjClYx",
        "outputId": "6c28978b-c07c-4cc5-db4d-589904423c39"
      },
      "id": "XyLMVCtjClYx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/SMU Work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.5041 - loss: 0.6960Epoch 1: Training Accuracy: 0.4902, Validation Accuracy: 0.4615\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 394ms/step - accuracy: 0.5013 - loss: 0.6959 - val_accuracy: 0.4615 - val_loss: 0.6950\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.5490 - loss: 0.6931Epoch 2: Training Accuracy: 0.5294, Validation Accuracy: 0.4615\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - accuracy: 0.5451 - loss: 0.6932 - val_accuracy: 0.4615 - val_loss: 0.6956\n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.5217 - loss: 0.6954Epoch 3: Training Accuracy: 0.5294, Validation Accuracy: 0.4231\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.5232 - loss: 0.6951 - val_accuracy: 0.4231 - val_loss: 0.6939\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.4643 - loss: 0.6963Epoch 4: Training Accuracy: 0.4510, Validation Accuracy: 0.3846\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - accuracy: 0.4616 - loss: 0.6962 - val_accuracy: 0.3846 - val_loss: 0.6939\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6002 - loss: 0.6893Epoch 5: Training Accuracy: 0.5882, Validation Accuracy: 0.3846\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 0.5978 - loss: 0.6892 - val_accuracy: 0.3846 - val_loss: 0.6943\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.5429 - loss: 0.6865Epoch 6: Training Accuracy: 0.5882, Validation Accuracy: 0.3846\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.5520 - loss: 0.6863 - val_accuracy: 0.3846 - val_loss: 0.6956\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.6720 - loss: 0.6823Epoch 7: Training Accuracy: 0.6569, Validation Accuracy: 0.3846\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.6690 - loss: 0.6827 - val_accuracy: 0.3846 - val_loss: 0.6966\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.5494 - loss: 0.6915Epoch 8: Training Accuracy: 0.5882, Validation Accuracy: 0.3846\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy: 0.5572 - loss: 0.6912 - val_accuracy: 0.3846 - val_loss: 0.6953\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.5665 - loss: 0.6861Epoch 9: Training Accuracy: 0.5784, Validation Accuracy: 0.5000\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.5689 - loss: 0.6861 - val_accuracy: 0.5000 - val_loss: 0.6943\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.5610 - loss: 0.6923Epoch 10: Training Accuracy: 0.5098, Validation Accuracy: 0.5000\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.5508 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6949\n",
            "Epoch 1/10\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.4635 - loss: 0.7054Epoch 1: Training Accuracy: 0.4608, Validation Accuracy: 0.6154\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 195ms/step - accuracy: 0.4624 - loss: 0.7037 - val_accuracy: 0.6154 - val_loss: 0.6921\n",
            "Epoch 2/10\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6545 - loss: 0.6809Epoch 2: Training Accuracy: 0.6275, Validation Accuracy: 0.5385\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.6437 - loss: 0.6815 - val_accuracy: 0.5385 - val_loss: 0.6912\n",
            "Epoch 3/10\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.6042 - loss: 0.6841Epoch 3: Training Accuracy: 0.6078, Validation Accuracy: 0.5769\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6056 - loss: 0.6821 - val_accuracy: 0.5769 - val_loss: 0.6903\n",
            "Epoch 4/10\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7205 - loss: 0.6672Epoch 4: Training Accuracy: 0.7059, Validation Accuracy: 0.5769\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7146 - loss: 0.6670 - val_accuracy: 0.5769 - val_loss: 0.6883\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.6986 - loss: 0.6601Epoch 5: Training Accuracy: 0.7059, Validation Accuracy: 0.5769\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 0.7001 - loss: 0.6600 - val_accuracy: 0.5769 - val_loss: 0.6854\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5921 - loss: 0.6616Epoch 6: Training Accuracy: 0.6078, Validation Accuracy: 0.5769\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 0.5952 - loss: 0.6617 - val_accuracy: 0.5769 - val_loss: 0.6829\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.6879 - loss: 0.6430Epoch 7: Training Accuracy: 0.7255, Validation Accuracy: 0.5769\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.6954 - loss: 0.6419 - val_accuracy: 0.5769 - val_loss: 0.6793\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.7271 - loss: 0.6267Epoch 8: Training Accuracy: 0.7157, Validation Accuracy: 0.6154\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.7248 - loss: 0.6268 - val_accuracy: 0.6154 - val_loss: 0.6752\n",
            "Epoch 9/10\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.7309 - loss: 0.6353Epoch 9: Training Accuracy: 0.8137, Validation Accuracy: 0.6154\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.7640 - loss: 0.6268 - val_accuracy: 0.6154 - val_loss: 0.6700\n",
            "Epoch 10/10\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.7865 - loss: 0.5817Epoch 10: Training Accuracy: 0.7549, Validation Accuracy: 0.5769\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.7738 - loss: 0.5885 - val_accuracy: 0.5769 - val_loss: 0.6657\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.6949\n",
            "LSTM Model Final Accuracy: 0.5000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5769 - loss: 0.6657\n",
            "CNN Model Final Accuracy: 0.5769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PkhntC_LEjDU",
      "metadata": {
        "id": "PkhntC_LEjDU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "OvtIZ4pbD6DH",
      "metadata": {
        "id": "OvtIZ4pbD6DH"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}